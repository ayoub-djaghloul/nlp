{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur du mot 'hello': [-1.008897 -2.308451 -1.194275 -4.362735 -2.566641  0.314524  2.26193\n",
      "  0.054648  1.564844 -0.460636  0.792198  1.368713  0.78904   0.023878\n",
      " -1.936209 -0.780074 -0.206716  1.281536  0.136857 -0.289974  0.343309\n",
      "  0.07578   1.463206  1.09872   2.726326 -2.116665  0.761616  1.469714\n",
      "  1.25744   0.550109  0.109379 -2.392751  3.322121  1.719002  3.541541\n",
      "  3.793948  1.122714 -0.523812 -0.055596  1.201102  1.378412 -0.55792\n",
      " -2.854434 -0.524985 -3.309722 -0.389988  0.938012 -0.784064 -0.417545\n",
      " -0.082834  2.830519 -0.120168  0.725053  1.539997  0.262444 -0.274206\n",
      " -0.680187  0.994122 -0.016498  0.76964   1.805622 -1.43584   0.346884\n",
      "  4.835496 -1.867374 -1.036754  3.820033 -2.141501 -0.833632  0.315947\n",
      " -1.105845  0.848341  0.776994  0.357851 -0.861111  0.366939 -1.105156\n",
      "  0.93178  -0.60639  -1.727933  1.057621  0.53754  -1.28147  -2.091967\n",
      "  0.352063 -2.116781 -1.415346  0.168833 -0.666945  0.59475   0.29088\n",
      "  1.932806  0.87297   0.789694 -1.182721  1.373498 -0.151582  0.971833\n",
      "  1.092018  0.838084 -0.420968 -1.018083  1.815029 -0.51072   0.266038\n",
      " -0.968394 -1.331038 -1.31537   1.583159  2.581931  0.521297  0.95444\n",
      " -0.137104  0.160658  2.170321  1.767607  0.676746  0.713049 -0.82168\n",
      "  0.914592 -2.497558  3.07474   0.178676  3.856671 -0.154814  2.022769\n",
      "  0.244235 -0.663178  2.118647  0.157353  1.95425  -1.544127 -2.111917\n",
      "  1.159798  0.201265 -0.885333 -0.910447  2.66564   0.514706  0.55289\n",
      " -1.140822  0.440602 -0.608855 -1.937298  0.536562 -2.935745 -1.704946\n",
      "  1.441226 -1.967131  2.579138 -1.645447 -1.120946  1.101451  1.109597\n",
      "  0.419982  0.825251 -1.625364  0.442138 -0.728012 -0.41016   0.142235\n",
      " -2.428148  0.139592 -1.615654  0.144462 -1.407016  0.399617 -1.278677\n",
      " -0.615815  1.96234   1.425909 -3.031413  1.193219 -1.138618  0.736095\n",
      "  3.09842   0.436526  0.70312   0.314236  1.333311 -1.566566  0.920972\n",
      "  0.175618 -2.399523 -3.665779  0.868129  2.06984   1.566378 -0.307899\n",
      " -1.528883  0.995289  1.738929 -0.536378 -0.082065  1.008371  0.17479\n",
      " -0.586092  1.509668  2.335393 -0.948153  0.484745 -0.287459  0.02674\n",
      "  1.923079 -0.833939 -1.582553  0.171307  0.903237  0.600609 -1.395303\n",
      "  0.853194  0.531409  0.276078 -1.10811  -0.316211 -0.785847 -1.827981\n",
      "  1.327896  0.847857  0.807385 -2.556619 -0.69821   0.815272  1.034415\n",
      "  2.137674  1.608291 -1.046017  2.023288 -0.186057 -1.202421 -1.454511\n",
      " -1.067128 -0.369343  1.796559  0.390947 -0.916154  1.022539  1.273545\n",
      " -3.337748 -1.343771  0.709108 -3.916027 -1.296888  0.319055  2.343475\n",
      " -1.91354  -0.192783  0.506014  1.506822  0.477322 -0.097857  0.176178\n",
      " -0.505493 -1.291207 -0.447858 -0.208582 -0.212973 -1.202021 -1.050056\n",
      " -0.024908 -0.128233  0.364386  2.419054  1.029398  0.463365  3.264183\n",
      "  1.320884  0.388911  2.808021 -1.085764 -2.876244 -0.451306 -1.010831\n",
      " -2.971694  1.091635 -1.888687 -3.036384  1.145404 -1.292646  1.006114\n",
      "  1.126793 -2.813588  1.247441 -1.225877 -0.982587 -1.219586 -0.852469\n",
      " -1.678341 -2.309407 -0.885314  0.15971  -1.358233 -0.244009  0.74595\n",
      " -0.446151 -1.490872 -0.322777  1.89056   0.53922   2.864799 -2.672858\n",
      "  0.316342 -1.322061 -0.150949  1.851633  0.499951 -1.398617 -1.050762\n",
      " -0.532836 -3.746859  3.780633 -1.158964 -0.203239 -1.829477  3.483067\n",
      " -0.549747  0.997633 -0.279116 -1.696163  0.987432  2.013392 -0.42713\n",
      " -1.831511 -0.68266   0.795205 -2.774332 -0.753904 -1.854837  2.391213\n",
      " -0.80639  -3.14955   5.203102 -1.366704  0.848538 -0.471454  1.19461\n",
      " -1.710024  1.31943   1.194559 -1.186882 -1.157679 -0.348767  3.868819\n",
      " -0.575271 -3.38658  -3.120433 -2.885225 -1.682682  0.958329  2.4061\n",
      " -1.091857 -0.461187  0.807513 -1.143139  1.268874 -4.077825  2.000273\n",
      " -0.577026  0.040827 -2.797378 -1.556533 -1.408881 -0.733918 -0.014902\n",
      "  1.020664 -0.209256 -2.343189  0.143903  2.937984 -1.786882 -0.221494\n",
      "  0.376469  0.426055 -1.863796 -0.34389   1.077638 -1.661607  2.57488\n",
      "  0.554016 -1.21701  -1.149036 -1.215664  0.856744  0.643309 -2.601124\n",
      "  0.401588 -0.665818 -1.181297  0.273477 -0.499626 -1.430754  4.122453\n",
      "  0.987856 -0.15448  -0.063721 -0.247651  0.167338  2.61871  -0.377334\n",
      " -0.996266]\n",
      "Similarité cosinus entre 'hello' et 'world': -0.028946194748510966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_word_vectors(file_path,word):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split(' ')\n",
    "            word = values[0]\n",
    "            if word == word:\n",
    "                vec=[float(val) for val in values[1:]]\n",
    "            vector = np.array([float(val) for val in values[1:]])\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors,vec\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus entre deux mots\n",
    "def calculate_cosine_similarity(word1, word2):\n",
    "    if word1 in word_vectors and word2 in word_vectors:\n",
    "        vec1 = word_vectors[word1]\n",
    "        vec2 = word_vectors[word2]\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        return similarity\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Exemple d'utilisation\n",
    "word1 = \"hello\"\n",
    "word2 = \"world\"\n",
    "\n",
    "# Chargement des vecteurs d'embedding\n",
    "word_vectors,vec = load_word_vectors('vectors400.txt',word1)\n",
    "\n",
    "similarity = calculate_cosine_similarity(word1, word2)\n",
    "\n",
    "if vector is not None:\n",
    "    print(f\"Vecteur du mot '{word1}': {vector}\")\n",
    "else:\n",
    "    print(f\"Le mot '{word1}' n'a pas été trouvé dans le fichier.\")\n",
    "\n",
    "if similarity is not None:\n",
    "    print(f\"Similarité cosinus entre '{word1}' et '{word2}': {similarity}\")\n",
    "else:\n",
    "    print(f\"Les vecteurs pour '{word1}' et/ou '{word2}' ne sont pas présents dans le fichier.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
